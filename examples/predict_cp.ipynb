{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "trained-creature",
   "metadata": {},
   "source": [
    "First, let's import everything we need, and load some cloud point data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alternative-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Graph Operator - handles data preparation, model creation/recall, hand-off of data to model\n",
    "from graphchem import GraphOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "helpful-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other dependencies are for data segmentation, set metric calculations, plotting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import median_absolute_error, r2_score\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "union-drilling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCCCCCOCCOCCOCCO', 'CCCCCCOCCOCCOCCOCCO', 'CCCCCCOCCOCCOCCOCCOCCO'] \n",
      " [[40.5], [63.8], [75.0]]\n"
     ]
    }
   ],
   "source": [
    "# Load some cloud point data\n",
    "from graphchem.datasets import load_cp\n",
    "smiles, cp = load_cp()\n",
    "print(smiles[:3], '\\n', cp[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "revolutionary-admission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 34 9 9\n"
     ]
    }
   ],
   "source": [
    "# Create training, testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    smiles, cp, test_size=0.20, random_state=42\n",
    ")\n",
    "print(len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-diversity",
   "metadata": {},
   "source": [
    "We need to set up some variables for our training process (i.e. hyper-parameters). In the future, these will be tunable to reduce model error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mobile-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    'task': 'graph',\n",
    "    'valid_size': 0.2,\n",
    "    'valid_epoch_iter': 1,\n",
    "    'valid_patience': 64,\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 0.001,\n",
    "    'lr_decay': 0.0000001,\n",
    "    'epochs': 500,\n",
    "    'verbose': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-frank",
   "metadata": {},
   "source": [
    "We also need to define our model's architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "precise-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'n_messages': 2,\n",
    "    'n_hidden': 3,\n",
    "    'hidden_msg_dim': 128,\n",
    "    'hidden_dim': 256,\n",
    "    'dropout': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-spider",
   "metadata": {},
   "source": [
    "Now let's initialize the Graph Operator, and train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "matched-incentive",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tjkessler/anaconda3/envs/torch_geometric/lib/python3.8/site-packages/graphchem-1.0.0-py3.8.egg/graphchem/operator.py:43: UserWarning: device config value not found: default value set, cpu\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 9823.79089807581 | Valid Loss: 2940.29296875\n",
      "Epoch: 1 | Train Loss: 3145.563774956597 | Valid Loss: 304.2599182128906\n",
      "Epoch: 2 | Train Loss: 765.0473587601273 | Valid Loss: 304.2599182128906\n",
      "Epoch: 3 | Train Loss: 554.9449869791666 | Valid Loss: 304.2599182128906\n",
      "Epoch: 4 | Train Loss: 769.2982381184896 | Valid Loss: 304.2599182128906\n",
      "Epoch: 5 | Train Loss: 841.2907127097801 | Valid Loss: 299.7872619628906\n",
      "Epoch: 6 | Train Loss: 568.6480712890625 | Valid Loss: 299.7872619628906\n",
      "Epoch: 7 | Train Loss: 651.2863226996528 | Valid Loss: 299.7872619628906\n",
      "Epoch: 8 | Train Loss: 464.5363340024595 | Valid Loss: 299.7872619628906\n",
      "Epoch: 9 | Train Loss: 485.8194122314453 | Valid Loss: 284.63104248046875\n",
      "Epoch: 10 | Train Loss: 514.3078703703703 | Valid Loss: 284.63104248046875\n",
      "Epoch: 11 | Train Loss: 506.75633635344326 | Valid Loss: 284.63104248046875\n",
      "Epoch: 12 | Train Loss: 475.68306025752315 | Valid Loss: 284.63104248046875\n",
      "Epoch: 13 | Train Loss: 433.72167629665796 | Valid Loss: 284.63104248046875\n",
      "Epoch: 14 | Train Loss: 463.236245614511 | Valid Loss: 284.63104248046875\n",
      "Epoch: 15 | Train Loss: 464.4170667860243 | Valid Loss: 284.63104248046875\n",
      "Epoch: 16 | Train Loss: 428.1498379177517 | Valid Loss: 266.1605529785156\n",
      "Epoch: 17 | Train Loss: 467.91851128472223 | Valid Loss: 266.1605529785156\n",
      "Epoch: 18 | Train Loss: 423.4028511047363 | Valid Loss: 266.1605529785156\n",
      "Epoch: 19 | Train Loss: 439.31829833984375 | Valid Loss: 266.1605529785156\n",
      "Epoch: 20 | Train Loss: 447.07456348560476 | Valid Loss: 266.1605529785156\n",
      "Epoch: 21 | Train Loss: 458.10986328125 | Valid Loss: 266.1605529785156\n",
      "Epoch: 22 | Train Loss: 445.05472366898147 | Valid Loss: 266.1605529785156\n",
      "Epoch: 23 | Train Loss: 509.6242901837384 | Valid Loss: 266.1605529785156\n",
      "Epoch: 24 | Train Loss: 448.9885609944661 | Valid Loss: 266.1605529785156\n",
      "Epoch: 25 | Train Loss: 448.8517908166956 | Valid Loss: 266.1605529785156\n",
      "Epoch: 26 | Train Loss: 407.81458960639105 | Valid Loss: 266.1605529785156\n",
      "Epoch: 27 | Train Loss: 523.2500813802084 | Valid Loss: 266.1605529785156\n",
      "Epoch: 28 | Train Loss: 553.286779333044 | Valid Loss: 266.1605529785156\n",
      "Epoch: 29 | Train Loss: 444.6541168778031 | Valid Loss: 266.1605529785156\n",
      "Epoch: 30 | Train Loss: 474.365155255353 | Valid Loss: 266.1605529785156\n",
      "Epoch: 31 | Train Loss: 599.7032154224537 | Valid Loss: 266.1605529785156\n",
      "Epoch: 32 | Train Loss: 544.1685169361256 | Valid Loss: 266.1605529785156\n",
      "Epoch: 33 | Train Loss: 600.158677842882 | Valid Loss: 266.1605529785156\n",
      "Epoch: 34 | Train Loss: 722.5840928819445 | Valid Loss: 266.1605529785156\n",
      "Epoch: 35 | Train Loss: 503.21841317635995 | Valid Loss: 266.1605529785156\n",
      "Epoch: 36 | Train Loss: 597.2465888129341 | Valid Loss: 266.1605529785156\n",
      "Epoch: 37 | Train Loss: 709.0129711009838 | Valid Loss: 266.1605529785156\n",
      "Epoch: 38 | Train Loss: 433.0311550564236 | Valid Loss: 266.1605529785156\n",
      "Epoch: 39 | Train Loss: 580.3864836516203 | Valid Loss: 266.1605529785156\n",
      "Epoch: 40 | Train Loss: 517.7840576171875 | Valid Loss: 266.1605529785156\n",
      "Epoch: 41 | Train Loss: 510.006354437934 | Valid Loss: 266.1605529785156\n",
      "Epoch: 42 | Train Loss: 573.250986735026 | Valid Loss: 266.1605529785156\n",
      "Epoch: 43 | Train Loss: 440.7332989728009 | Valid Loss: 266.1605529785156\n",
      "Epoch: 44 | Train Loss: 502.7800044307002 | Valid Loss: 266.1605529785156\n",
      "Epoch: 45 | Train Loss: 425.74664193612557 | Valid Loss: 266.1605529785156\n",
      "Epoch: 46 | Train Loss: 431.60436785662614 | Valid Loss: 265.55047607421875\n",
      "Epoch: 47 | Train Loss: 460.8819331416377 | Valid Loss: 265.55047607421875\n",
      "Epoch: 48 | Train Loss: 512.045809145327 | Valid Loss: 265.55047607421875\n",
      "Epoch: 49 | Train Loss: 493.49858940972223 | Valid Loss: 265.55047607421875\n",
      "Epoch: 50 | Train Loss: 446.61145471643516 | Valid Loss: 265.55047607421875\n",
      "Epoch: 51 | Train Loss: 474.6294479370117 | Valid Loss: 265.55047607421875\n",
      "Epoch: 52 | Train Loss: 429.1329583062066 | Valid Loss: 265.55047607421875\n",
      "Epoch: 53 | Train Loss: 422.13989822952834 | Valid Loss: 265.55047607421875\n",
      "Epoch: 54 | Train Loss: 431.7438580548322 | Valid Loss: 265.55047607421875\n",
      "Epoch: 55 | Train Loss: 404.05720774332684 | Valid Loss: 265.55047607421875\n",
      "Epoch: 56 | Train Loss: 452.98780059814453 | Valid Loss: 265.55047607421875\n",
      "Epoch: 57 | Train Loss: 438.4537424158167 | Valid Loss: 265.55047607421875\n",
      "Epoch: 58 | Train Loss: 428.2865255850333 | Valid Loss: 265.55047607421875\n",
      "Epoch: 59 | Train Loss: 422.19515652126734 | Valid Loss: 265.55047607421875\n",
      "Epoch: 60 | Train Loss: 414.4437204996745 | Valid Loss: 265.55047607421875\n",
      "Epoch: 61 | Train Loss: 461.25027804904516 | Valid Loss: 265.55047607421875\n",
      "Epoch: 62 | Train Loss: 468.22157118055554 | Valid Loss: 265.55047607421875\n",
      "Epoch: 63 | Train Loss: 465.6873055917245 | Valid Loss: 265.55047607421875\n",
      "Epoch: 64 | Train Loss: 409.0748946578414 | Valid Loss: 265.55047607421875\n",
      "Epoch: 65 | Train Loss: 523.7583821614584 | Valid Loss: 265.55047607421875\n",
      "Epoch: 66 | Train Loss: 412.7931145562066 | Valid Loss: 265.55047607421875\n",
      "Epoch: 67 | Train Loss: 419.68493878399886 | Valid Loss: 265.55047607421875\n",
      "Epoch: 68 | Train Loss: 468.1462877061632 | Valid Loss: 248.3928985595703\n",
      "Epoch: 69 | Train Loss: 408.9124405472367 | Valid Loss: 248.3928985595703\n",
      "Epoch: 70 | Train Loss: 493.8920412416811 | Valid Loss: 248.3928985595703\n",
      "Epoch: 71 | Train Loss: 408.73032633463544 | Valid Loss: 248.3928985595703\n",
      "Epoch: 72 | Train Loss: 492.5235912181713 | Valid Loss: 248.3928985595703\n",
      "Epoch: 73 | Train Loss: 384.54342877423323 | Valid Loss: 248.3928985595703\n",
      "Epoch: 74 | Train Loss: 433.2144317626953 | Valid Loss: 248.3928985595703\n",
      "Epoch: 75 | Train Loss: 402.72680353235313 | Valid Loss: 248.3928985595703\n",
      "Epoch: 76 | Train Loss: 383.7302743417245 | Valid Loss: 248.3928985595703\n",
      "Epoch: 77 | Train Loss: 403.30390082465277 | Valid Loss: 248.3928985595703\n",
      "Epoch: 78 | Train Loss: 386.5241478814019 | Valid Loss: 248.3928985595703\n",
      "Epoch: 79 | Train Loss: 378.7821768301505 | Valid Loss: 248.3928985595703\n",
      "Epoch: 80 | Train Loss: 396.19692088939524 | Valid Loss: 248.3928985595703\n",
      "Epoch: 81 | Train Loss: 363.25878680193864 | Valid Loss: 248.3928985595703\n",
      "Epoch: 82 | Train Loss: 356.0035988136574 | Valid Loss: 248.3928985595703\n",
      "Epoch: 83 | Train Loss: 544.179298683449 | Valid Loss: 248.3928985595703\n",
      "Epoch: 84 | Train Loss: 534.6232378924334 | Valid Loss: 248.3928985595703\n",
      "Epoch: 85 | Train Loss: 410.8998435691551 | Valid Loss: 240.2172088623047\n",
      "Epoch: 86 | Train Loss: 470.7208398889612 | Valid Loss: 240.2172088623047\n",
      "Epoch: 87 | Train Loss: 396.6965339095504 | Valid Loss: 240.2172088623047\n",
      "Epoch: 88 | Train Loss: 386.37131641529226 | Valid Loss: 240.2172088623047\n",
      "Epoch: 89 | Train Loss: 393.70855034722223 | Valid Loss: 220.66470336914062\n",
      "Epoch: 90 | Train Loss: 372.8546142578125 | Valid Loss: 220.66470336914062\n",
      "Epoch: 91 | Train Loss: 446.24729410807294 | Valid Loss: 220.02700805664062\n",
      "Epoch: 92 | Train Loss: 364.04103370949076 | Valid Loss: 220.02700805664062\n",
      "Epoch: 93 | Train Loss: 390.16774269386576 | Valid Loss: 220.02700805664062\n",
      "Epoch: 94 | Train Loss: 354.7275102403429 | Valid Loss: 220.02700805664062\n",
      "Epoch: 95 | Train Loss: 359.9461443865741 | Valid Loss: 220.02700805664062\n",
      "Epoch: 96 | Train Loss: 349.6444261338976 | Valid Loss: 220.02700805664062\n",
      "Epoch: 97 | Train Loss: 327.56299167209204 | Valid Loss: 220.02700805664062\n",
      "Epoch: 98 | Train Loss: 383.9347867612486 | Valid Loss: 220.02700805664062\n",
      "Epoch: 99 | Train Loss: 361.00059226707174 | Valid Loss: 199.81289672851562\n",
      "Epoch: 100 | Train Loss: 417.7333147967303 | Valid Loss: 199.81289672851562\n",
      "Epoch: 101 | Train Loss: 379.0570656105324 | Valid Loss: 199.81289672851562\n",
      "Epoch: 102 | Train Loss: 428.2601047092014 | Valid Loss: 199.36605834960938\n",
      "Epoch: 103 | Train Loss: 535.2924657751013 | Valid Loss: 199.36605834960938\n",
      "Epoch: 104 | Train Loss: 445.27120406539353 | Valid Loss: 199.36605834960938\n",
      "Epoch: 105 | Train Loss: 497.7372696487992 | Valid Loss: 199.36605834960938\n",
      "Epoch: 106 | Train Loss: 307.2017604686596 | Valid Loss: 199.36605834960938\n",
      "Epoch: 107 | Train Loss: 411.0520923755787 | Valid Loss: 199.36605834960938\n",
      "Epoch: 108 | Train Loss: 440.42716019241897 | Valid Loss: 199.36605834960938\n",
      "Epoch: 109 | Train Loss: 484.5341163917824 | Valid Loss: 199.36605834960938\n",
      "Epoch: 110 | Train Loss: 368.73367648654516 | Valid Loss: 199.36605834960938\n",
      "Epoch: 111 | Train Loss: 335.84280734592016 | Valid Loss: 199.36605834960938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 112 | Train Loss: 549.5567073115596 | Valid Loss: 199.36605834960938\n",
      "Epoch: 113 | Train Loss: 392.5541178385417 | Valid Loss: 199.36605834960938\n",
      "Epoch: 114 | Train Loss: 374.69854736328125 | Valid Loss: 199.36605834960938\n",
      "Epoch: 115 | Train Loss: 379.01911304615163 | Valid Loss: 199.36605834960938\n",
      "Epoch: 116 | Train Loss: 381.858376538312 | Valid Loss: 199.36605834960938\n",
      "Epoch: 117 | Train Loss: 322.9126038727937 | Valid Loss: 199.36605834960938\n",
      "Epoch: 118 | Train Loss: 415.83428842050057 | Valid Loss: 199.36605834960938\n",
      "Epoch: 119 | Train Loss: 543.028322007921 | Valid Loss: 199.36605834960938\n",
      "Epoch: 120 | Train Loss: 357.8740980360243 | Valid Loss: 199.36605834960938\n",
      "Epoch: 121 | Train Loss: 424.6994890283655 | Valid Loss: 199.36605834960938\n",
      "Epoch: 122 | Train Loss: 380.96386492693864 | Valid Loss: 199.36605834960938\n",
      "Epoch: 123 | Train Loss: 483.7409860116464 | Valid Loss: 199.36605834960938\n",
      "Epoch: 124 | Train Loss: 473.20372970015916 | Valid Loss: 199.36605834960938\n",
      "Epoch: 125 | Train Loss: 405.7276679144965 | Valid Loss: 199.36605834960938\n",
      "Epoch: 126 | Train Loss: 342.4643574467412 | Valid Loss: 199.36605834960938\n",
      "Epoch: 127 | Train Loss: 470.7041628802264 | Valid Loss: 199.36605834960938\n",
      "Epoch: 128 | Train Loss: 365.930258291739 | Valid Loss: 199.36605834960938\n",
      "Epoch: 129 | Train Loss: 367.5552164713542 | Valid Loss: 199.36605834960938\n",
      "Epoch: 130 | Train Loss: 357.4373779296875 | Valid Loss: 199.36605834960938\n",
      "Epoch: 131 | Train Loss: 364.2962578667535 | Valid Loss: 199.36605834960938\n",
      "Epoch: 132 | Train Loss: 461.3386038321036 | Valid Loss: 199.36605834960938\n",
      "Epoch: 133 | Train Loss: 370.21280246310766 | Valid Loss: 199.36605834960938\n",
      "Epoch: 134 | Train Loss: 381.69522829409 | Valid Loss: 199.36605834960938\n",
      "Epoch: 135 | Train Loss: 328.9449400725188 | Valid Loss: 199.36605834960938\n",
      "Epoch: 136 | Train Loss: 387.6978652388961 | Valid Loss: 199.36605834960938\n",
      "Epoch: 137 | Train Loss: 473.682534677011 | Valid Loss: 199.02842712402344\n",
      "Epoch: 138 | Train Loss: 324.7083016854745 | Valid Loss: 183.35484313964844\n",
      "Epoch: 139 | Train Loss: 345.4878924334491 | Valid Loss: 183.35484313964844\n",
      "Epoch: 140 | Train Loss: 406.0600732873987 | Valid Loss: 183.35484313964844\n",
      "Epoch: 141 | Train Loss: 554.893465395327 | Valid Loss: 183.35484313964844\n",
      "Epoch: 142 | Train Loss: 408.36786566840277 | Valid Loss: 183.35484313964844\n",
      "Epoch: 143 | Train Loss: 416.9626894350405 | Valid Loss: 183.35484313964844\n",
      "Epoch: 144 | Train Loss: 486.54845965350114 | Valid Loss: 183.35484313964844\n",
      "Epoch: 145 | Train Loss: 440.69481376365377 | Valid Loss: 183.35484313964844\n",
      "Epoch: 146 | Train Loss: 340.73968929714624 | Valid Loss: 183.35484313964844\n",
      "Epoch: 147 | Train Loss: 328.9597653989439 | Valid Loss: 183.35484313964844\n",
      "Epoch: 148 | Train Loss: 344.49594737865306 | Valid Loss: 183.35484313964844\n",
      "Epoch: 149 | Train Loss: 395.65333839699076 | Valid Loss: 183.35484313964844\n",
      "Epoch: 150 | Train Loss: 304.3281074806496 | Valid Loss: 183.35484313964844\n",
      "Epoch: 151 | Train Loss: 279.9186457881221 | Valid Loss: 179.11727905273438\n",
      "Epoch: 152 | Train Loss: 261.179520783601 | Valid Loss: 179.11727905273438\n",
      "Epoch: 153 | Train Loss: 274.7496137265806 | Valid Loss: 179.11727905273438\n",
      "Epoch: 154 | Train Loss: 549.9090677897135 | Valid Loss: 179.11727905273438\n",
      "Epoch: 155 | Train Loss: 406.6118621826172 | Valid Loss: 179.11727905273438\n",
      "Epoch: 156 | Train Loss: 423.51428674768516 | Valid Loss: 179.11727905273438\n",
      "Epoch: 157 | Train Loss: 372.4818623860677 | Valid Loss: 179.11727905273438\n",
      "Epoch: 158 | Train Loss: 382.6706130416305 | Valid Loss: 179.11727905273438\n",
      "Epoch: 159 | Train Loss: 405.53526023582174 | Valid Loss: 179.11727905273438\n",
      "Epoch: 160 | Train Loss: 394.9466552734375 | Valid Loss: 179.11727905273438\n",
      "Epoch: 161 | Train Loss: 301.410816333912 | Valid Loss: 171.21875\n",
      "Epoch: 162 | Train Loss: 305.17861825448495 | Valid Loss: 171.21875\n",
      "Epoch: 163 | Train Loss: 269.01166449652777 | Valid Loss: 171.21875\n",
      "Epoch: 164 | Train Loss: 250.31401288067852 | Valid Loss: 171.21875\n",
      "Epoch: 165 | Train Loss: 256.9681345621745 | Valid Loss: 171.21875\n",
      "Epoch: 166 | Train Loss: 313.7042711046007 | Valid Loss: 123.80500030517578\n",
      "Epoch: 167 | Train Loss: 352.2673848470052 | Valid Loss: 123.80500030517578\n",
      "Epoch: 168 | Train Loss: 210.77367259837962 | Valid Loss: 123.80500030517578\n",
      "Epoch: 169 | Train Loss: 201.06546246563946 | Valid Loss: 123.80500030517578\n",
      "Epoch: 170 | Train Loss: 242.3415244773582 | Valid Loss: 105.22503662109375\n",
      "Epoch: 171 | Train Loss: 120.36037473325376 | Valid Loss: 105.22503662109375\n",
      "Epoch: 172 | Train Loss: 588.5315483940972 | Valid Loss: 105.22503662109375\n",
      "Epoch: 173 | Train Loss: 604.0812807436342 | Valid Loss: 105.22503662109375\n",
      "Epoch: 174 | Train Loss: 243.68757431595415 | Valid Loss: 105.22503662109375\n",
      "Epoch: 175 | Train Loss: 420.1898035120081 | Valid Loss: 105.22503662109375\n",
      "Epoch: 176 | Train Loss: 246.22500779893664 | Valid Loss: 105.22503662109375\n",
      "Epoch: 177 | Train Loss: 292.5312115704572 | Valid Loss: 105.22503662109375\n",
      "Epoch: 178 | Train Loss: 207.79758043642397 | Valid Loss: 105.22503662109375\n",
      "Epoch: 179 | Train Loss: 309.3359126338252 | Valid Loss: 105.22503662109375\n",
      "Epoch: 180 | Train Loss: 296.2441168891059 | Valid Loss: 105.22503662109375\n",
      "Epoch: 181 | Train Loss: 267.4557370786314 | Valid Loss: 105.22503662109375\n",
      "Epoch: 182 | Train Loss: 232.54579784252024 | Valid Loss: 105.22503662109375\n",
      "Epoch: 183 | Train Loss: 255.63084920247397 | Valid Loss: 102.84278869628906\n",
      "Epoch: 184 | Train Loss: 169.20891994900174 | Valid Loss: 95.91178131103516\n",
      "Epoch: 185 | Train Loss: 137.6755133734809 | Valid Loss: 95.91178131103516\n",
      "Epoch: 186 | Train Loss: 194.16507127549914 | Valid Loss: 82.33537292480469\n",
      "Epoch: 187 | Train Loss: 136.98787434895834 | Valid Loss: 82.33537292480469\n",
      "Epoch: 188 | Train Loss: 109.40109125773112 | Valid Loss: 69.15015411376953\n",
      "Epoch: 189 | Train Loss: 99.65500443070023 | Valid Loss: 69.15015411376953\n",
      "Epoch: 190 | Train Loss: 176.68759211787471 | Valid Loss: 69.15015411376953\n",
      "Epoch: 191 | Train Loss: 231.61855118362993 | Valid Loss: 69.15015411376953\n",
      "Epoch: 192 | Train Loss: 220.24272311175312 | Valid Loss: 69.15015411376953\n",
      "Epoch: 193 | Train Loss: 115.02060671205874 | Valid Loss: 69.15015411376953\n",
      "Epoch: 194 | Train Loss: 128.73335506297923 | Valid Loss: 69.15015411376953\n",
      "Epoch: 195 | Train Loss: 110.00175334789135 | Valid Loss: 69.15015411376953\n",
      "Epoch: 196 | Train Loss: 166.8248986138238 | Valid Loss: 61.15696334838867\n",
      "Epoch: 197 | Train Loss: 200.49493577745227 | Valid Loss: 61.15696334838867\n",
      "Epoch: 198 | Train Loss: 213.13202469437212 | Valid Loss: 61.15696334838867\n",
      "Epoch: 199 | Train Loss: 124.21674262152777 | Valid Loss: 61.15696334838867\n",
      "Epoch: 200 | Train Loss: 207.66775399667245 | Valid Loss: 61.15696334838867\n",
      "Epoch: 201 | Train Loss: 120.12319409405744 | Valid Loss: 61.15696334838867\n",
      "Epoch: 202 | Train Loss: 129.79452306252938 | Valid Loss: 61.15696334838867\n",
      "Epoch: 203 | Train Loss: 287.7671056676794 | Valid Loss: 61.15696334838867\n",
      "Epoch: 204 | Train Loss: 188.53312626591435 | Valid Loss: 61.15696334838867\n",
      "Epoch: 205 | Train Loss: 169.26123894585504 | Valid Loss: 61.15696334838867\n",
      "Epoch: 206 | Train Loss: 203.99737803141275 | Valid Loss: 61.15696334838867\n",
      "Epoch: 207 | Train Loss: 186.40992397732205 | Valid Loss: 61.15696334838867\n",
      "Epoch: 208 | Train Loss: 101.27433268229167 | Valid Loss: 61.15696334838867\n",
      "Epoch: 209 | Train Loss: 241.78076341417102 | Valid Loss: 61.15696334838867\n",
      "Epoch: 210 | Train Loss: 247.9637247721354 | Valid Loss: 61.15696334838867\n",
      "Epoch: 211 | Train Loss: 193.32546728628654 | Valid Loss: 61.15696334838867\n",
      "Epoch: 212 | Train Loss: 231.49142229998554 | Valid Loss: 61.15696334838867\n",
      "Epoch: 213 | Train Loss: 208.93956445764613 | Valid Loss: 61.15696334838867\n",
      "Epoch: 214 | Train Loss: 105.91439141167535 | Valid Loss: 61.15696334838867\n",
      "Epoch: 215 | Train Loss: 204.01043701171875 | Valid Loss: 61.15696334838867\n",
      "Epoch: 216 | Train Loss: 116.16382175021701 | Valid Loss: 61.15696334838867\n",
      "Epoch: 217 | Train Loss: 106.45258726897063 | Valid Loss: 61.15696334838867\n",
      "Epoch: 218 | Train Loss: 142.032595316569 | Valid Loss: 61.15696334838867\n",
      "Epoch: 219 | Train Loss: 89.58399680808738 | Valid Loss: 61.15696334838867\n",
      "Epoch: 220 | Train Loss: 94.39271630181207 | Valid Loss: 48.052494049072266\n",
      "Epoch: 221 | Train Loss: 67.41686121622722 | Valid Loss: 46.60895538330078\n",
      "Epoch: 222 | Train Loss: 85.02785181116175 | Valid Loss: 46.60895538330078\n",
      "Epoch: 223 | Train Loss: 84.4586883827492 | Valid Loss: 46.60895538330078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 224 | Train Loss: 333.3748474121094 | Valid Loss: 46.60895538330078\n",
      "Epoch: 225 | Train Loss: 110.16744373462818 | Valid Loss: 46.60895538330078\n",
      "Epoch: 226 | Train Loss: 131.4993142022027 | Valid Loss: 46.60895538330078\n",
      "Epoch: 227 | Train Loss: 101.00182653356481 | Valid Loss: 46.60895538330078\n",
      "Epoch: 228 | Train Loss: 107.7585523393419 | Valid Loss: 46.60895538330078\n",
      "Epoch: 229 | Train Loss: 84.7030903145119 | Valid Loss: 46.60895538330078\n",
      "Epoch: 230 | Train Loss: 71.71619556568287 | Valid Loss: 46.60895538330078\n",
      "Epoch: 231 | Train Loss: 224.63322279188367 | Valid Loss: 46.60895538330078\n",
      "Epoch: 232 | Train Loss: 147.04429287380643 | Valid Loss: 46.60895538330078\n",
      "Epoch: 233 | Train Loss: 104.6241359004268 | Valid Loss: 46.60895538330078\n",
      "Epoch: 234 | Train Loss: 118.36395602756076 | Valid Loss: 46.60895538330078\n",
      "Epoch: 235 | Train Loss: 130.7840347290039 | Valid Loss: 46.60895538330078\n",
      "Epoch: 236 | Train Loss: 95.90229966905382 | Valid Loss: 46.60895538330078\n",
      "Epoch: 237 | Train Loss: 98.06137480559173 | Valid Loss: 46.60895538330078\n",
      "Epoch: 238 | Train Loss: 146.1827848928946 | Valid Loss: 46.60895538330078\n",
      "Epoch: 239 | Train Loss: 94.98045857747395 | Valid Loss: 46.60895538330078\n",
      "Epoch: 240 | Train Loss: 104.70321203161168 | Valid Loss: 46.60895538330078\n",
      "Epoch: 241 | Train Loss: 106.13128379539206 | Valid Loss: 39.51968765258789\n",
      "Epoch: 242 | Train Loss: 92.93630642361111 | Valid Loss: 39.51968765258789\n",
      "Epoch: 243 | Train Loss: 114.20146913881655 | Valid Loss: 39.51968765258789\n",
      "Epoch: 244 | Train Loss: 95.77229026511863 | Valid Loss: 39.51968765258789\n",
      "Epoch: 245 | Train Loss: 85.50608034487124 | Valid Loss: 39.51968765258789\n",
      "Epoch: 246 | Train Loss: 167.70996376320167 | Valid Loss: 39.51968765258789\n",
      "Epoch: 247 | Train Loss: 89.46305741204156 | Valid Loss: 39.51968765258789\n",
      "Epoch: 248 | Train Loss: 86.16729566786024 | Valid Loss: 39.51968765258789\n",
      "Epoch: 249 | Train Loss: 78.08342516863787 | Valid Loss: 39.51968765258789\n",
      "Epoch: 250 | Train Loss: 73.63170793321397 | Valid Loss: 39.51968765258789\n",
      "Epoch: 251 | Train Loss: 97.9901710792824 | Valid Loss: 39.51968765258789\n",
      "Epoch: 252 | Train Loss: 77.22649553087022 | Valid Loss: 30.263242721557617\n",
      "Epoch: 253 | Train Loss: 109.27983305189345 | Valid Loss: 30.263242721557617\n",
      "Epoch: 254 | Train Loss: 101.84478816279659 | Valid Loss: 30.263242721557617\n",
      "Epoch: 255 | Train Loss: 124.1406701405843 | Valid Loss: 30.263242721557617\n",
      "Epoch: 256 | Train Loss: 118.1124030219184 | Valid Loss: 30.263242721557617\n",
      "Epoch: 257 | Train Loss: 217.03403614185476 | Valid Loss: 30.263242721557617\n",
      "Epoch: 258 | Train Loss: 292.97530167191115 | Valid Loss: 30.263242721557617\n",
      "Epoch: 259 | Train Loss: 303.3721624303747 | Valid Loss: 30.263242721557617\n",
      "Epoch: 260 | Train Loss: 232.96503476743345 | Valid Loss: 30.263242721557617\n",
      "Epoch: 261 | Train Loss: 156.13727258752894 | Valid Loss: 30.263242721557617\n",
      "Epoch: 262 | Train Loss: 107.94547554298684 | Valid Loss: 30.263242721557617\n",
      "Epoch: 263 | Train Loss: 122.54988352457683 | Valid Loss: 30.263242721557617\n",
      "Epoch: 264 | Train Loss: 126.27513970269098 | Valid Loss: 30.263242721557617\n",
      "Epoch: 265 | Train Loss: 105.97878138224284 | Valid Loss: 30.263242721557617\n",
      "Epoch: 266 | Train Loss: 134.11807420518664 | Valid Loss: 30.263242721557617\n",
      "Epoch: 267 | Train Loss: 75.87781213831019 | Valid Loss: 30.263242721557617\n",
      "Epoch: 268 | Train Loss: 84.25894705454509 | Valid Loss: 30.263242721557617\n",
      "Epoch: 269 | Train Loss: 67.42228896529586 | Valid Loss: 30.263242721557617\n",
      "Epoch: 270 | Train Loss: 84.9520342791522 | Valid Loss: 30.263242721557617\n",
      "Epoch: 271 | Train Loss: 111.38839806450738 | Valid Loss: 30.263242721557617\n",
      "Epoch: 272 | Train Loss: 92.47140361644604 | Valid Loss: 30.263242721557617\n",
      "Epoch: 273 | Train Loss: 82.29225102177372 | Valid Loss: 30.263242721557617\n",
      "Epoch: 274 | Train Loss: 96.60991923014323 | Valid Loss: 30.263242721557617\n",
      "Epoch: 275 | Train Loss: 116.66006526240596 | Valid Loss: 30.263242721557617\n",
      "Epoch: 276 | Train Loss: 157.0823449028863 | Valid Loss: 30.263242721557617\n",
      "Epoch: 277 | Train Loss: 117.38710191514757 | Valid Loss: 30.263242721557617\n",
      "Epoch: 278 | Train Loss: 240.57477089210792 | Valid Loss: 30.263242721557617\n",
      "Epoch: 279 | Train Loss: 236.36639185304995 | Valid Loss: 30.263242721557617\n",
      "Epoch: 280 | Train Loss: 129.517904776114 | Valid Loss: 30.263242721557617\n",
      "Epoch: 281 | Train Loss: 114.38695978235316 | Valid Loss: 30.263242721557617\n",
      "Epoch: 282 | Train Loss: 107.829551131637 | Valid Loss: 30.263242721557617\n",
      "Epoch: 283 | Train Loss: 120.6324809745506 | Valid Loss: 30.263242721557617\n",
      "Epoch: 284 | Train Loss: 101.04004739831996 | Valid Loss: 30.263242721557617\n",
      "Epoch: 285 | Train Loss: 89.70533413357205 | Valid Loss: 30.263242721557617\n",
      "Epoch: 286 | Train Loss: 253.45797672978154 | Valid Loss: 30.263242721557617\n",
      "Epoch: 287 | Train Loss: 160.2270112214265 | Valid Loss: 30.263242721557617\n",
      "Epoch: 288 | Train Loss: 99.7821730154532 | Valid Loss: 30.263242721557617\n",
      "Epoch: 289 | Train Loss: 116.47599227340133 | Valid Loss: 30.263242721557617\n",
      "Epoch: 290 | Train Loss: 157.1256832546658 | Valid Loss: 30.263242721557617\n",
      "Epoch: 291 | Train Loss: 136.63997056749133 | Valid Loss: 30.263242721557617\n",
      "Epoch: 292 | Train Loss: 102.25903971989949 | Valid Loss: 30.263242721557617\n",
      "Epoch: 293 | Train Loss: 143.44992969654226 | Valid Loss: 30.263242721557617\n",
      "Epoch: 294 | Train Loss: 152.78382534450955 | Valid Loss: 30.263242721557617\n",
      "Epoch: 295 | Train Loss: 190.39131447120948 | Valid Loss: 30.263242721557617\n",
      "Epoch: 296 | Train Loss: 239.5125820018627 | Valid Loss: 30.263242721557617\n",
      "Epoch: 297 | Train Loss: 203.8404620135272 | Valid Loss: 30.263242721557617\n",
      "Epoch: 298 | Train Loss: 144.1422576904297 | Valid Loss: 30.263242721557617\n",
      "Epoch: 299 | Train Loss: 156.45290346498842 | Valid Loss: 30.263242721557617\n",
      "Epoch: 300 | Train Loss: 110.9977874049434 | Valid Loss: 30.263242721557617\n",
      "Epoch: 301 | Train Loss: 150.16162052860966 | Valid Loss: 30.263242721557617\n",
      "Epoch: 302 | Train Loss: 91.41466550473814 | Valid Loss: 30.263242721557617\n",
      "Epoch: 303 | Train Loss: 124.71323818630643 | Valid Loss: 30.263242721557617\n",
      "Epoch: 304 | Train Loss: 104.92546265213578 | Valid Loss: 30.263242721557617\n",
      "Epoch: 305 | Train Loss: 92.0054264775029 | Valid Loss: 30.263242721557617\n",
      "Epoch: 306 | Train Loss: 98.90000519929109 | Valid Loss: 30.263242721557617\n",
      "Epoch: 307 | Train Loss: 111.28587228280527 | Valid Loss: 30.263242721557617\n",
      "Epoch: 308 | Train Loss: 103.04448947200069 | Valid Loss: 30.263242721557617\n",
      "Epoch: 309 | Train Loss: 175.8240763346354 | Valid Loss: 30.263242721557617\n",
      "Epoch: 310 | Train Loss: 65.69147109985352 | Valid Loss: 30.263242721557617\n",
      "Epoch: 311 | Train Loss: 187.11321145516854 | Valid Loss: 30.263242721557617\n",
      "Epoch: 312 | Train Loss: 113.57795446890371 | Valid Loss: 30.263242721557617\n",
      "Epoch: 313 | Train Loss: 84.76716839825666 | Valid Loss: 30.263242721557617\n",
      "Epoch: 314 | Train Loss: 123.28709990889938 | Valid Loss: 30.263242721557617\n",
      "Epoch: 315 | Train Loss: 98.80348134923864 | Valid Loss: 30.263242721557617\n",
      "Epoch: 316 | Train Loss: 80.44053803549872 | Valid Loss: 30.263242721557617\n"
     ]
    }
   ],
   "source": [
    "go = GraphOperator(config=training_config)\n",
    "go.train(X_train, y_train, model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-malawi",
   "metadata": {},
   "source": [
    "Let's get predictions, then calculate prediction errors for the training and testing subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extended-charlotte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set: R2 = 0.7109448620627039, MAE = 14.156970977783203\n",
      "Train Set: R2 = 0.8580065110245041, MAE = 8.13055591583252\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = go.use(X_train)\n",
    "y_test_pred = go.use(X_test)\n",
    "\n",
    "mae_test = median_absolute_error(y_test, y_test_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "mae_train = median_absolute_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print('Test Set: R2 = {}, MAE = {}'.format(r2_test, mae_test))\n",
    "print('Train Set: R2 = {}, MAE = {}'.format(r2_train, mae_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-stomach",
   "metadata": {},
   "source": [
    "And let's plot our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "second-rocket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Predicted Cloud Point vs. Experimental Cloud Point')\n",
    "plt.xlabel('Experimental Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.scatter(y_train, y_train_pred, color='blue', label='Train')\n",
    "plt.scatter(y_test, y_test_pred, color='red', label='Test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-array",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
