{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "trained-creature",
   "metadata": {},
   "source": [
    "First, let's import everything we need, and load some YSI data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alternative-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Graph Operator - handles data preparation, model creation/recall, hand-off of data to model\n",
    "from graphchem import GraphOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "helpful-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other dependencies are for data segmentation, set metric calculations, plotting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import median_absolute_error, r2_score\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "union-drilling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C1=CC=C(C=C1)C2=CC=CC=C2C3=CC=CC=C3', 'C1C2=CC=CC3=C2C4=C(C=CC=C41)C=C3', 'C1=CC=C2C(=C1)C3=CC=CC4=C3C2=CC=C4'] \n",
      " [[1338.9], [1312.8], [1291.9]]\n"
     ]
    }
   ],
   "source": [
    "# Load some YSI data\n",
    "from graphchem.datasets import load_ysi\n",
    "smiles, ysi = load_ysi()\n",
    "print(smiles[:3], '\\n', ysi[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "revolutionary-admission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446 446 112 112\n"
     ]
    }
   ],
   "source": [
    "# Create training, testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    smiles, ysi, test_size=0.20, random_state=42\n",
    ")\n",
    "print(len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-diversity",
   "metadata": {},
   "source": [
    "We need to set up some variables for our training process (i.e. hyper-parameters). In the future, these will be tunable to reduce model error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mobile-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    'task': 'graph',\n",
    "    'valid_size': 0.2,\n",
    "    'valid_epoch_iter': 1,\n",
    "    'valid_patience': 48,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.002,\n",
    "    'lr_decay': 0.0000001,\n",
    "    'epochs': 500,\n",
    "    'verbose': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-frank",
   "metadata": {},
   "source": [
    "We also need to define our model's architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "precise-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'n_messages': 2,\n",
    "    'n_hidden': 3,\n",
    "    'hidden_msg_dim': 128,\n",
    "    'hidden_dim': 256,\n",
    "    'dropout': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-spider",
   "metadata": {},
   "source": [
    "Now let's initialize the Graph Operator, and train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "matched-incentive",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tjkessler/anaconda3/envs/torch_geometric/lib/python3.8/site-packages/graphchem-1.0.0-py3.8.egg/graphchem/operator.py:43: UserWarning: device config value not found: default value set, cpu\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 38400.84146769663 | Valid Loss: 21589.615234375\n",
      "Epoch: 1 | Train Loss: 23746.834522033005 | Valid Loss: 18544.61328125\n",
      "Epoch: 2 | Train Loss: 26017.959203827246 | Valid Loss: 12660.8955078125\n",
      "Epoch: 3 | Train Loss: 12653.213417310393 | Valid Loss: 12660.8955078125\n",
      "Epoch: 4 | Train Loss: 16475.90363851826 | Valid Loss: 7374.1728515625\n",
      "Epoch: 5 | Train Loss: 8889.460586376405 | Valid Loss: 5574.98876953125\n",
      "Epoch: 6 | Train Loss: 7540.7394169153795 | Valid Loss: 5574.98876953125\n",
      "Epoch: 7 | Train Loss: 7297.227034322332 | Valid Loss: 4473.40185546875\n",
      "Epoch: 8 | Train Loss: 7105.450162394663 | Valid Loss: 4473.40185546875\n",
      "Epoch: 9 | Train Loss: 11618.5263671875 | Valid Loss: 4287.10546875\n",
      "Epoch: 10 | Train Loss: 7585.573938948385 | Valid Loss: 4121.013671875\n",
      "Epoch: 11 | Train Loss: 6767.7960575403795 | Valid Loss: 4121.013671875\n",
      "Epoch: 12 | Train Loss: 7388.98423235604 | Valid Loss: 3922.760498046875\n",
      "Epoch: 13 | Train Loss: 6383.562121444874 | Valid Loss: 3605.503173828125\n",
      "Epoch: 14 | Train Loss: 8034.827924749825 | Valid Loss: 3605.503173828125\n",
      "Epoch: 15 | Train Loss: 6127.728369552098 | Valid Loss: 3605.503173828125\n",
      "Epoch: 16 | Train Loss: 6202.629897899842 | Valid Loss: 3605.503173828125\n",
      "Epoch: 17 | Train Loss: 9673.222849642292 | Valid Loss: 3605.503173828125\n",
      "Epoch: 18 | Train Loss: 11873.539216116573 | Valid Loss: 3605.503173828125\n",
      "Epoch: 19 | Train Loss: 8397.155745259832 | Valid Loss: 3605.503173828125\n",
      "Epoch: 20 | Train Loss: 8544.99869425913 | Valid Loss: 3605.503173828125\n",
      "Epoch: 21 | Train Loss: 10744.939902187733 | Valid Loss: 3605.503173828125\n",
      "Epoch: 22 | Train Loss: 6367.015712438005 | Valid Loss: 3605.503173828125\n",
      "Epoch: 23 | Train Loss: 6186.089356325985 | Valid Loss: 3322.43603515625\n",
      "Epoch: 24 | Train Loss: 6451.106914048784 | Valid Loss: 2960.94970703125\n",
      "Epoch: 25 | Train Loss: 5896.944785814607 | Valid Loss: 2960.94970703125\n",
      "Epoch: 26 | Train Loss: 7730.321142646704 | Valid Loss: 2960.94970703125\n",
      "Epoch: 27 | Train Loss: 5462.251777563202 | Valid Loss: 2960.94970703125\n",
      "Epoch: 28 | Train Loss: 10555.49273887377 | Valid Loss: 2960.94970703125\n",
      "Epoch: 29 | Train Loss: 6352.724632006013 | Valid Loss: 2960.94970703125\n",
      "Epoch: 30 | Train Loss: 5856.674376755618 | Valid Loss: 2960.94970703125\n",
      "Epoch: 31 | Train Loss: 6408.172286472964 | Valid Loss: 2960.94970703125\n",
      "Epoch: 32 | Train Loss: 7646.437258602528 | Valid Loss: 2960.94970703125\n",
      "Epoch: 33 | Train Loss: 11579.693072715501 | Valid Loss: 2960.94970703125\n",
      "Epoch: 34 | Train Loss: 5900.351619420427 | Valid Loss: 2960.94970703125\n",
      "Epoch: 35 | Train Loss: 6522.428982509656 | Valid Loss: 2960.94970703125\n",
      "Epoch: 36 | Train Loss: 7790.227319610252 | Valid Loss: 2960.94970703125\n",
      "Epoch: 37 | Train Loss: 6824.772201709533 | Valid Loss: 2960.94970703125\n",
      "Epoch: 38 | Train Loss: 6745.14195765806 | Valid Loss: 2960.94970703125\n",
      "Epoch: 39 | Train Loss: 7225.2090255222965 | Valid Loss: 2960.94970703125\n",
      "Epoch: 40 | Train Loss: 5328.3110461288625 | Valid Loss: 2960.94970703125\n",
      "Epoch: 41 | Train Loss: 5751.996335147472 | Valid Loss: 2960.94970703125\n",
      "Epoch: 42 | Train Loss: 8584.308821431707 | Valid Loss: 2960.94970703125\n",
      "Epoch: 43 | Train Loss: 5435.876174069523 | Valid Loss: 2960.94970703125\n",
      "Epoch: 44 | Train Loss: 7133.58426357655 | Valid Loss: 2960.94970703125\n",
      "Epoch: 45 | Train Loss: 9407.32196387816 | Valid Loss: 2960.94970703125\n",
      "Epoch: 46 | Train Loss: 10408.678494228407 | Valid Loss: 2960.94970703125\n",
      "Epoch: 47 | Train Loss: 9783.909684427668 | Valid Loss: 2960.94970703125\n",
      "Epoch: 48 | Train Loss: 6122.522306635139 | Valid Loss: 2893.226806640625\n",
      "Epoch: 49 | Train Loss: 5341.033436635907 | Valid Loss: 2687.710693359375\n",
      "Epoch: 50 | Train Loss: 5249.871055345857 | Valid Loss: 2687.710693359375\n",
      "Epoch: 51 | Train Loss: 4640.508749115334 | Valid Loss: 2687.710693359375\n",
      "Epoch: 52 | Train Loss: 5698.760908148262 | Valid Loss: 2687.710693359375\n",
      "Epoch: 53 | Train Loss: 5910.597135050913 | Valid Loss: 2687.710693359375\n",
      "Epoch: 54 | Train Loss: 5864.840085147472 | Valid Loss: 2687.710693359375\n",
      "Epoch: 55 | Train Loss: 7560.94763869382 | Valid Loss: 2687.710693359375\n",
      "Epoch: 56 | Train Loss: 9506.89727440309 | Valid Loss: 2687.710693359375\n",
      "Epoch: 57 | Train Loss: 13850.8907127809 | Valid Loss: 2687.710693359375\n",
      "Epoch: 58 | Train Loss: 11655.63770354196 | Valid Loss: 2687.710693359375\n",
      "Epoch: 59 | Train Loss: 5331.9173954310045 | Valid Loss: 2687.710693359375\n",
      "Epoch: 60 | Train Loss: 5764.96056031645 | Valid Loss: 2687.710693359375\n",
      "Epoch: 61 | Train Loss: 4441.1610217148 | Valid Loss: 2687.710693359375\n",
      "Epoch: 62 | Train Loss: 4620.168055159323 | Valid Loss: 2294.410400390625\n",
      "Epoch: 63 | Train Loss: 4410.152826544944 | Valid Loss: 2294.410400390625\n",
      "Epoch: 64 | Train Loss: 6744.6949613764045 | Valid Loss: 2294.410400390625\n",
      "Epoch: 65 | Train Loss: 8038.120800232619 | Valid Loss: 2294.410400390625\n",
      "Epoch: 66 | Train Loss: 6327.105316505003 | Valid Loss: 2294.410400390625\n",
      "Epoch: 67 | Train Loss: 5059.743332080627 | Valid Loss: 2294.410400390625\n",
      "Epoch: 68 | Train Loss: 5627.31298828125 | Valid Loss: 2294.410400390625\n",
      "Epoch: 69 | Train Loss: 6465.702661407127 | Valid Loss: 2294.410400390625\n",
      "Epoch: 70 | Train Loss: 5182.228954529494 | Valid Loss: 2294.410400390625\n",
      "Epoch: 71 | Train Loss: 6958.397268916784 | Valid Loss: 2212.104736328125\n",
      "Epoch: 72 | Train Loss: 4365.6867934666325 | Valid Loss: 2212.104736328125\n",
      "Epoch: 73 | Train Loss: 4214.643126755618 | Valid Loss: 2212.104736328125\n",
      "Epoch: 74 | Train Loss: 7954.900792496927 | Valid Loss: 2212.104736328125\n",
      "Epoch: 75 | Train Loss: 4905.732323550106 | Valid Loss: 2212.104736328125\n",
      "Epoch: 76 | Train Loss: 6043.050642034981 | Valid Loss: 2212.104736328125\n",
      "Epoch: 77 | Train Loss: 4557.134655898876 | Valid Loss: 2212.104736328125\n",
      "Epoch: 78 | Train Loss: 7187.919559778792 | Valid Loss: 2212.104736328125\n",
      "Epoch: 79 | Train Loss: 9153.974598402388 | Valid Loss: 2212.104736328125\n",
      "Epoch: 80 | Train Loss: 11441.18618945861 | Valid Loss: 2212.104736328125\n",
      "Epoch: 81 | Train Loss: 5222.1620984023875 | Valid Loss: 2212.104736328125\n",
      "Epoch: 82 | Train Loss: 10910.28970988413 | Valid Loss: 2212.104736328125\n",
      "Epoch: 83 | Train Loss: 5426.125266771638 | Valid Loss: 2212.104736328125\n",
      "Epoch: 84 | Train Loss: 5720.542963263694 | Valid Loss: 2212.104736328125\n",
      "Epoch: 85 | Train Loss: 5076.359457294593 | Valid Loss: 2212.104736328125\n",
      "Epoch: 86 | Train Loss: 4842.687184451671 | Valid Loss: 2212.104736328125\n",
      "Epoch: 87 | Train Loss: 5161.464158433207 | Valid Loss: 2212.104736328125\n",
      "Epoch: 88 | Train Loss: 5070.565171831109 | Valid Loss: 2212.104736328125\n",
      "Epoch: 89 | Train Loss: 4737.187527431531 | Valid Loss: 2212.104736328125\n",
      "Epoch: 90 | Train Loss: 4129.502924201194 | Valid Loss: 2212.104736328125\n",
      "Epoch: 91 | Train Loss: 4139.373845132549 | Valid Loss: 2212.104736328125\n",
      "Epoch: 92 | Train Loss: 5906.699807156338 | Valid Loss: 2212.104736328125\n",
      "Epoch: 93 | Train Loss: 4914.265842223435 | Valid Loss: 2212.104736328125\n",
      "Epoch: 94 | Train Loss: 7295.2638271203205 | Valid Loss: 2212.104736328125\n",
      "Epoch: 95 | Train Loss: 9221.73474806882 | Valid Loss: 2212.104736328125\n",
      "Epoch: 96 | Train Loss: 8548.306272013804 | Valid Loss: 2137.6884765625\n",
      "Epoch: 97 | Train Loss: 4377.384092180916 | Valid Loss: 2137.6884765625\n",
      "Epoch: 98 | Train Loss: 4077.9630332689608 | Valid Loss: 2137.6884765625\n",
      "Epoch: 99 | Train Loss: 6630.7396089360955 | Valid Loss: 2137.6884765625\n",
      "Epoch: 100 | Train Loss: 7281.336971668715 | Valid Loss: 2137.6884765625\n",
      "Epoch: 101 | Train Loss: 4750.72067860807 | Valid Loss: 2137.6884765625\n",
      "Epoch: 102 | Train Loss: 5052.308224795909 | Valid Loss: 2137.6884765625\n",
      "Epoch: 103 | Train Loss: 5222.62804489993 | Valid Loss: 2137.6884765625\n",
      "Epoch: 104 | Train Loss: 7623.501902376668 | Valid Loss: 2137.6884765625\n",
      "Epoch: 105 | Train Loss: 5354.934600487184 | Valid Loss: 2137.6884765625\n",
      "Epoch: 106 | Train Loss: 5140.593948878599 | Valid Loss: 2137.6884765625\n",
      "Epoch: 107 | Train Loss: 5512.975332195839 | Valid Loss: 2137.6884765625\n",
      "Epoch: 108 | Train Loss: 5652.395679259568 | Valid Loss: 2137.6884765625\n",
      "Epoch: 109 | Train Loss: 3973.389900807584 | Valid Loss: 2137.6884765625\n",
      "Epoch: 110 | Train Loss: 8083.295204419768 | Valid Loss: 2137.6884765625\n",
      "Epoch: 111 | Train Loss: 6617.8058231653795 | Valid Loss: 2137.6884765625\n",
      "Epoch: 112 | Train Loss: 4719.896879389045 | Valid Loss: 1919.37158203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113 | Train Loss: 3865.1623740892733 | Valid Loss: 1919.37158203125\n",
      "Epoch: 114 | Train Loss: 3927.8612713760203 | Valid Loss: 1919.37158203125\n",
      "Epoch: 115 | Train Loss: 4180.772639242451 | Valid Loss: 1919.37158203125\n",
      "Epoch: 116 | Train Loss: 4532.9098490168535 | Valid Loss: 1919.37158203125\n",
      "Epoch: 117 | Train Loss: 3455.625173504433 | Valid Loss: 1919.37158203125\n",
      "Epoch: 118 | Train Loss: 3972.2196500971077 | Valid Loss: 1919.37158203125\n",
      "Epoch: 119 | Train Loss: 3764.9346147172905 | Valid Loss: 1919.37158203125\n",
      "Epoch: 120 | Train Loss: 4260.873085107696 | Valid Loss: 1919.37158203125\n",
      "Epoch: 121 | Train Loss: 3097.04778024052 | Valid Loss: 1919.37158203125\n",
      "Epoch: 122 | Train Loss: 4017.7672544329353 | Valid Loss: 1919.37158203125\n",
      "Epoch: 123 | Train Loss: 3409.102013062895 | Valid Loss: 1919.37158203125\n",
      "Epoch: 124 | Train Loss: 4264.18218856983 | Valid Loss: 1919.37158203125\n",
      "Epoch: 125 | Train Loss: 5428.302056087537 | Valid Loss: 1919.37158203125\n",
      "Epoch: 126 | Train Loss: 3594.8576234967522 | Valid Loss: 1919.37158203125\n",
      "Epoch: 127 | Train Loss: 3283.9250995764573 | Valid Loss: 1919.37158203125\n",
      "Epoch: 128 | Train Loss: 3537.7291739817415 | Valid Loss: 1919.37158203125\n",
      "Epoch: 129 | Train Loss: 10908.512190572332 | Valid Loss: 1919.37158203125\n",
      "Epoch: 130 | Train Loss: 6008.978054775281 | Valid Loss: 1919.37158203125\n",
      "Epoch: 131 | Train Loss: 10144.898507450403 | Valid Loss: 1919.37158203125\n",
      "Epoch: 132 | Train Loss: 5641.509507768609 | Valid Loss: 1919.37158203125\n",
      "Epoch: 133 | Train Loss: 4756.0825634217 | Valid Loss: 1854.068115234375\n",
      "Epoch: 134 | Train Loss: 4400.632702773876 | Valid Loss: 1854.068115234375\n",
      "Epoch: 135 | Train Loss: 5037.5457612798455 | Valid Loss: 1854.068115234375\n",
      "Epoch: 136 | Train Loss: 3562.8627384057204 | Valid Loss: 1854.068115234375\n",
      "Epoch: 137 | Train Loss: 4221.023579715343 | Valid Loss: 1854.068115234375\n",
      "Epoch: 138 | Train Loss: 4598.266821014748 | Valid Loss: 1854.068115234375\n",
      "Epoch: 139 | Train Loss: 5684.2117110691715 | Valid Loss: 1854.068115234375\n",
      "Epoch: 140 | Train Loss: 3900.333295072063 | Valid Loss: 1854.068115234375\n",
      "Epoch: 141 | Train Loss: 3073.709731541323 | Valid Loss: 1854.068115234375\n",
      "Epoch: 142 | Train Loss: 3889.391674256057 | Valid Loss: 1854.068115234375\n",
      "Epoch: 143 | Train Loss: 5564.603625351124 | Valid Loss: 1854.068115234375\n",
      "Epoch: 144 | Train Loss: 7716.058911955758 | Valid Loss: 1854.068115234375\n",
      "Epoch: 145 | Train Loss: 4349.9821516744205 | Valid Loss: 1854.068115234375\n",
      "Epoch: 146 | Train Loss: 6557.407988644718 | Valid Loss: 1854.068115234375\n",
      "Epoch: 147 | Train Loss: 8840.826053233628 | Valid Loss: 1854.068115234375\n",
      "Epoch: 148 | Train Loss: 6447.25874517205 | Valid Loss: 1854.068115234375\n",
      "Epoch: 149 | Train Loss: 6997.3838402394495 | Valid Loss: 1854.068115234375\n",
      "Epoch: 150 | Train Loss: 3820.833491293232 | Valid Loss: 1854.068115234375\n",
      "Epoch: 151 | Train Loss: 5347.779632225465 | Valid Loss: 1854.068115234375\n",
      "Epoch: 152 | Train Loss: 5073.817992821168 | Valid Loss: 1854.068115234375\n",
      "Epoch: 153 | Train Loss: 4200.851165342867 | Valid Loss: 1854.068115234375\n",
      "Epoch: 154 | Train Loss: 3376.68024710323 | Valid Loss: 1612.2850341796875\n",
      "Epoch: 155 | Train Loss: 3094.902288544044 | Valid Loss: 1612.2850341796875\n",
      "Epoch: 156 | Train Loss: 3335.8967946941934 | Valid Loss: 1612.2850341796875\n",
      "Epoch: 157 | Train Loss: 3114.7768499824438 | Valid Loss: 1612.2850341796875\n",
      "Epoch: 158 | Train Loss: 8645.204507549157 | Valid Loss: 1612.2850341796875\n",
      "Epoch: 159 | Train Loss: 5317.062817348523 | Valid Loss: 1612.2850341796875\n",
      "Epoch: 160 | Train Loss: 4920.546617807967 | Valid Loss: 1612.2850341796875\n",
      "Epoch: 161 | Train Loss: 3680.0545483278424 | Valid Loss: 1587.4095458984375\n",
      "Epoch: 162 | Train Loss: 3182.447522452708 | Valid Loss: 1587.4095458984375\n",
      "Epoch: 163 | Train Loss: 4350.184970469957 | Valid Loss: 1587.4095458984375\n",
      "Epoch: 164 | Train Loss: 3803.3333589360955 | Valid Loss: 1587.4095458984375\n",
      "Epoch: 165 | Train Loss: 2737.0861692964363 | Valid Loss: 1587.4095458984375\n",
      "Epoch: 166 | Train Loss: 3777.464977864469 | Valid Loss: 1587.4095458984375\n",
      "Epoch: 167 | Train Loss: 4673.536868234699 | Valid Loss: 1587.4095458984375\n",
      "Epoch: 168 | Train Loss: 2694.955244428656 | Valid Loss: 1587.4095458984375\n",
      "Epoch: 169 | Train Loss: 2660.269285480628 | Valid Loss: 1574.72998046875\n",
      "Epoch: 170 | Train Loss: 2358.8249100245785 | Valid Loss: 1574.72998046875\n",
      "Epoch: 171 | Train Loss: 3961.135377348139 | Valid Loss: 1574.72998046875\n",
      "Epoch: 172 | Train Loss: 3254.5378106792323 | Valid Loss: 1574.72998046875\n",
      "Epoch: 173 | Train Loss: 3410.0182913448034 | Valid Loss: 1574.72998046875\n",
      "Epoch: 174 | Train Loss: 3400.578841648745 | Valid Loss: 1574.72998046875\n",
      "Epoch: 175 | Train Loss: 2651.149547105425 | Valid Loss: 1574.72998046875\n",
      "Epoch: 176 | Train Loss: 2603.667306278529 | Valid Loss: 1574.72998046875\n",
      "Epoch: 177 | Train Loss: 2923.7699598676704 | Valid Loss: 1574.72998046875\n",
      "Epoch: 178 | Train Loss: 3144.9315722604815 | Valid Loss: 1574.72998046875\n",
      "Epoch: 179 | Train Loss: 2844.3206135610517 | Valid Loss: 1574.72998046875\n",
      "Epoch: 180 | Train Loss: 2435.4682549680215 | Valid Loss: 1574.72998046875\n",
      "Epoch: 181 | Train Loss: 2340.213502348139 | Valid Loss: 1574.72998046875\n",
      "Epoch: 182 | Train Loss: 2657.204378620962 | Valid Loss: 1574.72998046875\n",
      "Epoch: 183 | Train Loss: 2885.360082733497 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 184 | Train Loss: 2487.7573132461375 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 185 | Train Loss: 2195.0292694434693 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 186 | Train Loss: 3885.796479471614 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 187 | Train Loss: 4007.862934969784 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 188 | Train Loss: 2364.6812718423566 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 189 | Train Loss: 2413.6839949361392 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 190 | Train Loss: 1800.6826459906074 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 191 | Train Loss: 2507.4616232882727 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 192 | Train Loss: 3368.019853570488 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 193 | Train Loss: 8031.4122684778795 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 194 | Train Loss: 10186.927084247718 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 195 | Train Loss: 6925.480435832163 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 196 | Train Loss: 7983.784103093522 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 197 | Train Loss: 4141.017686479547 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 198 | Train Loss: 3089.1081200074614 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 199 | Train Loss: 3668.8843438652125 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 200 | Train Loss: 2203.0460301088483 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 201 | Train Loss: 1996.4789819610253 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 202 | Train Loss: 5660.806115311183 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 203 | Train Loss: 3387.7583282127807 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 204 | Train Loss: 2908.682665192679 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 205 | Train Loss: 2902.372073055653 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 206 | Train Loss: 3007.296818765362 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 207 | Train Loss: 2482.381028078915 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 208 | Train Loss: 3161.2737166158267 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 209 | Train Loss: 2130.3003111421394 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 210 | Train Loss: 1988.7837243240872 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 211 | Train Loss: 3310.88420602177 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 212 | Train Loss: 1938.3901147735253 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 213 | Train Loss: 2596.5486347327073 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 214 | Train Loss: 4003.2734169692135 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 215 | Train Loss: 3842.9909558242625 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 216 | Train Loss: 2380.5930059197244 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 217 | Train Loss: 2407.644941351387 | Valid Loss: 1446.1036376953125\n",
      "Epoch: 218 | Train Loss: 1652.2592859161034 | Valid Loss: 1409.5693359375\n",
      "Epoch: 219 | Train Loss: 1675.5673312069325 | Valid Loss: 1409.5693359375\n",
      "Epoch: 220 | Train Loss: 3088.3761521242977 | Valid Loss: 1409.5693359375\n",
      "Epoch: 221 | Train Loss: 16595.29124604986 | Valid Loss: 1409.5693359375\n",
      "Epoch: 222 | Train Loss: 8118.335180389748 | Valid Loss: 1409.5693359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 223 | Train Loss: 4016.0391591961466 | Valid Loss: 1409.5693359375\n",
      "Epoch: 224 | Train Loss: 3077.8621423271265 | Valid Loss: 1409.5693359375\n",
      "Epoch: 225 | Train Loss: 2994.337946859638 | Valid Loss: 1409.5693359375\n",
      "Epoch: 226 | Train Loss: 2806.9850882198034 | Valid Loss: 1409.5693359375\n",
      "Epoch: 227 | Train Loss: 5561.392325754916 | Valid Loss: 1409.5693359375\n",
      "Epoch: 228 | Train Loss: 3562.872959094101 | Valid Loss: 1409.5693359375\n",
      "Epoch: 229 | Train Loss: 3803.3857664472603 | Valid Loss: 1409.5693359375\n",
      "Epoch: 230 | Train Loss: 3949.5824752979065 | Valid Loss: 1409.5693359375\n",
      "Epoch: 231 | Train Loss: 2076.770142973139 | Valid Loss: 1409.5693359375\n",
      "Epoch: 232 | Train Loss: 1853.414369733146 | Valid Loss: 1409.5693359375\n",
      "Epoch: 233 | Train Loss: 1906.2130977330582 | Valid Loss: 1409.5693359375\n",
      "Epoch: 234 | Train Loss: 1769.2974393608865 | Valid Loss: 1409.5693359375\n",
      "Epoch: 235 | Train Loss: 1866.325071870611 | Valid Loss: 1409.5693359375\n",
      "Epoch: 236 | Train Loss: 2064.7105479722613 | Valid Loss: 1409.5693359375\n",
      "Epoch: 237 | Train Loss: 2983.6335257198034 | Valid Loss: 1409.5693359375\n",
      "Epoch: 238 | Train Loss: 5811.835347850671 | Valid Loss: 1409.5693359375\n",
      "Epoch: 239 | Train Loss: 4627.237882807014 | Valid Loss: 1342.6356201171875\n",
      "Epoch: 240 | Train Loss: 4648.9212646484375 | Valid Loss: 1342.6356201171875\n",
      "Epoch: 241 | Train Loss: 4928.018529999123 | Valid Loss: 1342.6356201171875\n",
      "Epoch: 242 | Train Loss: 5183.374262649022 | Valid Loss: 1342.6356201171875\n",
      "Epoch: 243 | Train Loss: 5098.457415291433 | Valid Loss: 1342.6356201171875\n",
      "Epoch: 244 | Train Loss: 2162.4805990497716 | Valid Loss: 1342.6356201171875\n",
      "Epoch: 245 | Train Loss: 1546.9489139599746 | Valid Loss: 1342.6356201171875\n",
      "Epoch: 246 | Train Loss: 1600.5512352418364 | Valid Loss: 1342.6356201171875\n",
      "Epoch: 247 | Train Loss: 2635.1790949789324 | Valid Loss: 1342.6356201171875\n",
      "Epoch: 248 | Train Loss: 1539.336554752307 | Valid Loss: 1260.425537109375\n",
      "Epoch: 249 | Train Loss: 1474.66650390625 | Valid Loss: 1260.425537109375\n",
      "Epoch: 250 | Train Loss: 2196.235991060064 | Valid Loss: 1260.425537109375\n",
      "Epoch: 251 | Train Loss: 1783.8089983650807 | Valid Loss: 1260.425537109375\n",
      "Epoch: 252 | Train Loss: 1351.4931016557673 | Valid Loss: 1260.425537109375\n",
      "Epoch: 253 | Train Loss: 1550.2824899051966 | Valid Loss: 1260.425537109375\n",
      "Epoch: 254 | Train Loss: 1453.881775588132 | Valid Loss: 1260.425537109375\n",
      "Epoch: 255 | Train Loss: 1196.9575466198867 | Valid Loss: 1260.425537109375\n",
      "Epoch: 256 | Train Loss: 1575.6775515267018 | Valid Loss: 1260.425537109375\n",
      "Epoch: 257 | Train Loss: 1364.9417381715239 | Valid Loss: 1260.425537109375\n",
      "Epoch: 258 | Train Loss: 1331.3366037433068 | Valid Loss: 1260.425537109375\n",
      "Epoch: 259 | Train Loss: 1469.6679694357883 | Valid Loss: 1260.425537109375\n",
      "Epoch: 260 | Train Loss: 1474.2492538623596 | Valid Loss: 1260.425537109375\n",
      "Epoch: 261 | Train Loss: 1795.4144822024227 | Valid Loss: 1260.425537109375\n",
      "Epoch: 262 | Train Loss: 1849.7512695655394 | Valid Loss: 1260.425537109375\n",
      "Epoch: 263 | Train Loss: 1333.0207128631935 | Valid Loss: 1260.425537109375\n",
      "Epoch: 264 | Train Loss: 1843.150414541866 | Valid Loss: 1260.425537109375\n",
      "Epoch: 265 | Train Loss: 1427.9817696903528 | Valid Loss: 1260.425537109375\n",
      "Epoch: 266 | Train Loss: 1496.5370627413974 | Valid Loss: 1260.425537109375\n",
      "Epoch: 267 | Train Loss: 1272.992475445351 | Valid Loss: 1260.425537109375\n",
      "Epoch: 268 | Train Loss: 1079.9839956733617 | Valid Loss: 1260.425537109375\n",
      "Epoch: 269 | Train Loss: 1135.4949114510182 | Valid Loss: 1260.425537109375\n",
      "Epoch: 270 | Train Loss: 2164.9677213175914 | Valid Loss: 1260.425537109375\n",
      "Epoch: 271 | Train Loss: 1604.9752265844452 | Valid Loss: 1260.425537109375\n",
      "Epoch: 272 | Train Loss: 1374.1671554051088 | Valid Loss: 1260.425537109375\n",
      "Epoch: 273 | Train Loss: 1786.9108428955078 | Valid Loss: 1260.425537109375\n",
      "Epoch: 274 | Train Loss: 1201.3531177499322 | Valid Loss: 1260.425537109375\n",
      "Epoch: 275 | Train Loss: 1118.1041462073165 | Valid Loss: 1260.425537109375\n",
      "Epoch: 276 | Train Loss: 1700.414243548104 | Valid Loss: 1260.425537109375\n",
      "Epoch: 277 | Train Loss: 1352.3821129959622 | Valid Loss: 1260.425537109375\n",
      "Epoch: 278 | Train Loss: 2240.648437157106 | Valid Loss: 1260.425537109375\n",
      "Epoch: 279 | Train Loss: 2616.3778885402035 | Valid Loss: 1260.425537109375\n",
      "Epoch: 280 | Train Loss: 1905.6578087967434 | Valid Loss: 1260.425537109375\n",
      "Epoch: 281 | Train Loss: 1757.5292447550912 | Valid Loss: 1260.425537109375\n",
      "Epoch: 282 | Train Loss: 6126.599586058199 | Valid Loss: 1260.425537109375\n",
      "Epoch: 283 | Train Loss: 4921.787985126624 | Valid Loss: 1260.425537109375\n",
      "Epoch: 284 | Train Loss: 4680.128499749001 | Valid Loss: 1260.425537109375\n",
      "Epoch: 285 | Train Loss: 5361.230150544242 | Valid Loss: 1260.425537109375\n",
      "Epoch: 286 | Train Loss: 2975.045349806882 | Valid Loss: 1260.425537109375\n",
      "Epoch: 287 | Train Loss: 2680.06963768434 | Valid Loss: 1260.425537109375\n",
      "Epoch: 288 | Train Loss: 2067.9116804144355 | Valid Loss: 1260.425537109375\n",
      "Epoch: 289 | Train Loss: 1737.629457795218 | Valid Loss: 1260.425537109375\n",
      "Epoch: 290 | Train Loss: 1751.1360658795645 | Valid Loss: 1260.425537109375\n",
      "Epoch: 291 | Train Loss: 1372.3265131403891 | Valid Loss: 1260.425537109375\n",
      "Epoch: 292 | Train Loss: 1309.0883480457778 | Valid Loss: 1260.425537109375\n",
      "Epoch: 293 | Train Loss: 2728.599080975136 | Valid Loss: 1260.425537109375\n",
      "Epoch: 294 | Train Loss: 1575.3455385358145 | Valid Loss: 1260.425537109375\n",
      "Epoch: 295 | Train Loss: 1623.234661659498 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 296 | Train Loss: 1531.500759853406 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 297 | Train Loss: 2153.72649117802 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 298 | Train Loss: 2388.4780918078477 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 299 | Train Loss: 2844.3197199789324 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 300 | Train Loss: 5950.506874341643 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 301 | Train Loss: 2315.840299113413 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 302 | Train Loss: 3072.203503212232 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 303 | Train Loss: 7220.978746049859 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 304 | Train Loss: 11373.78424518028 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 305 | Train Loss: 3013.463228718618 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 306 | Train Loss: 2178.064359857795 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 307 | Train Loss: 2047.0196603148172 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 308 | Train Loss: 1439.1148355891196 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 309 | Train Loss: 1263.395530443513 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 310 | Train Loss: 1053.165010945181 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 311 | Train Loss: 1854.4019769389977 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 312 | Train Loss: 1920.166700727484 | Valid Loss: 1186.5159912109375\n",
      "Epoch: 313 | Train Loss: 1893.9947300600202 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 314 | Train Loss: 1836.7016496979788 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 315 | Train Loss: 1691.7022801088483 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 316 | Train Loss: 1519.1821604525105 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 317 | Train Loss: 1387.963208916482 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 318 | Train Loss: 1293.0329178370787 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 319 | Train Loss: 2049.359161034059 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 320 | Train Loss: 2608.649624942394 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 321 | Train Loss: 1874.846526413821 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 322 | Train Loss: 1770.6445771978142 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 323 | Train Loss: 2623.505789424596 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 324 | Train Loss: 3346.141563158357 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 325 | Train Loss: 2182.9431363223644 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 326 | Train Loss: 1354.8382902681158 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 327 | Train Loss: 1581.6224392665906 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 328 | Train Loss: 1909.0444143916784 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 329 | Train Loss: 1546.9269306311446 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 330 | Train Loss: 2549.260696925474 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 331 | Train Loss: 2221.543923367275 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 332 | Train Loss: 2333.735510665379 | Valid Loss: 1174.1947021484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 333 | Train Loss: 1813.0974944039676 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 334 | Train Loss: 1822.8472502633426 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 335 | Train Loss: 2140.262034212605 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 336 | Train Loss: 2974.5214967191887 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 337 | Train Loss: 1657.5021441170338 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 338 | Train Loss: 1609.8121886521244 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 339 | Train Loss: 1385.271531694391 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 340 | Train Loss: 1223.738261019246 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 341 | Train Loss: 1268.611225256759 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 342 | Train Loss: 1224.1833948714009 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 343 | Train Loss: 2442.9313320202773 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 344 | Train Loss: 1497.75473193908 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 345 | Train Loss: 986.8727454710543 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 346 | Train Loss: 1484.220162981012 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 347 | Train Loss: 1439.6464019089603 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 348 | Train Loss: 1294.317334807321 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 349 | Train Loss: 2174.792115629389 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 350 | Train Loss: 2068.092809184214 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 351 | Train Loss: 1402.6510157210103 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 352 | Train Loss: 1552.5758207514045 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 353 | Train Loss: 1554.7196319237185 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 354 | Train Loss: 1837.1652612579003 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 355 | Train Loss: 1715.1643944215239 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 356 | Train Loss: 2526.2771517292836 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 357 | Train Loss: 3396.197917809647 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 358 | Train Loss: 3633.0476087934517 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 359 | Train Loss: 3109.3971262728232 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 360 | Train Loss: 2056.200762459401 | Valid Loss: 1174.1947021484375\n",
      "Epoch: 361 | Train Loss: 2763.743825505289 | Valid Loss: 1174.1947021484375\n"
     ]
    }
   ],
   "source": [
    "go = GraphOperator(config=training_config)\n",
    "go.train(X_train, y_train, model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-malawi",
   "metadata": {},
   "source": [
    "Let's get predictions, then calculate prediction errors for the training and testing subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extended-charlotte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set: R2 = 0.9597596029972522, MAE = 12.177635803222655\n",
      "Train Set: R2 = 0.9772958121622056, MAE = 10.357080268859864\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = go.use(X_train)\n",
    "y_test_pred = go.use(X_test)\n",
    "\n",
    "mae_test = median_absolute_error(y_test, y_test_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "mae_train = median_absolute_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print('Test Set: R2 = {}, MAE = {}'.format(r2_test, mae_test))\n",
    "print('Train Set: R2 = {}, MAE = {}'.format(r2_train, mae_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-stomach",
   "metadata": {},
   "source": [
    "And let's plot our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "second-rocket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Predicted YSI vs. Experimental YSI')\n",
    "plt.xlabel('Experimental Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.scatter(y_train, y_train_pred, color='blue', label='Train')\n",
    "plt.scatter(y_test, y_test_pred, color='red', label='Test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-array",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
